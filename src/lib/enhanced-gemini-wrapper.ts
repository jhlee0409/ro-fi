/**
 * ğŸš€ Enhanced Gemini API Wrapper
 * v2.1 ì°½ì˜ì„± ëª¨ë“œ ê¸°ë°˜ ê³ ë„í™”ëœ Gemini API ê´€ë¦¬ ì‹œìŠ¤í…œ
 *
 * Features:
 * - í† í° ìµœì í™” ë° ë¹„ìš© ê´€ë¦¬
 * - ì°½ì˜ì„± ëª¨ë“œ v2.1 í†µí•©
 * - ì¸í…”ë¦¬ì „íŠ¸ ìºì‹± ì‹œìŠ¤í…œ
 * - ì—ëŸ¬ í•¸ë“¤ë§ ë° ì¬ì‹œë„ ë¡œì§
 * - ë°°ì¹˜ ì²˜ë¦¬ ë° í ê´€ë¦¬
 */

import { GoogleGenerativeAI, GenerativeModel } from '@google/generative-ai';
import { TokenBalancingEngine } from './token-balancing-engine.js';

/**
 * API ìš”ì²­ ì„¤ì •
 */
interface APIRequestConfig {
  creativityMode: 'standard' | 'high' | 'unlimited';
  priority: 'low' | 'normal' | 'high' | 'critical';
  cacheStrategy: 'none' | 'memory' | 'persistent';
  retryPolicy: RetryPolicy;
  tokenLimit?: number;
  timeoutMs?: number;
}

interface RetryPolicy {
  maxRetries: number;
  baseDelayMs: number;
  exponentialBackoff: boolean;
  retryConditions: string[];
}

interface APIResponse<T = string> {
  success: boolean;
  data?: T;
  error?: Error;
  metadata: {
    tokensUsed: number;
    responseTime: number;
    fromCache: boolean;
    creativity: number;
    retryCount: number;
  };
}

interface CacheEntry {
  content: string;
  timestamp: number;
  tokensUsed: number;
  ttl: number;
  accessCount: number;
}

/**
 * í–¥ìƒëœ Gemini API ë˜í¼
 */
export class EnhancedGeminiWrapper {
  private genAI: GoogleGenerativeAI;
  private model: GenerativeModel;
  private tokenBalancer: TokenBalancingEngine;
  private cache: Map<string, CacheEntry>;
  private requestQueue: Array<QueuedRequest>;
  private isProcessingQueue: boolean;
  private rateLimiter: RateLimiter;
  private metrics: APIMetrics;

  constructor(apiKey: string, modelName: string = 'gemini-2.5-pro') {
    this.genAI = new GoogleGenerativeAI(apiKey);
    this.model = this.genAI.getGenerativeModel({ model: modelName });
    this.tokenBalancer = new TokenBalancingEngine();
    this.cache = new Map();
    this.requestQueue = [];
    this.isProcessingQueue = false;
    this.rateLimiter = new RateLimiter();
    this.metrics = new APIMetrics();
  }

  /**
   * ìŠ¤ë§ˆíŠ¸ ì½˜í…ì¸  ìƒì„± (v2.1 ì°½ì˜ì„± ëª¨ë“œ í†µí•©)
   */
  async generateContent(
    prompt: string,
    config: Partial<APIRequestConfig> = {}
  ): Promise<APIResponse> {
    const fullConfig = this.buildConfig(config);
    const cacheKey = this.generateCacheKey(prompt, fullConfig);

    // 1. ìºì‹œ í™•ì¸
    const cachedResult = this.checkCache(cacheKey, fullConfig.cacheStrategy);
    if (cachedResult) {
      return cachedResult;
    }

    // 2. í† í° ë°¸ëŸ°ì‹± ë° ë¹„ìš© ìµœì í™”
    const optimizedPrompt = await this.optimizePrompt(prompt, fullConfig);

    // 3. ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ… í™•ì¸
    await this.rateLimiter.waitIfNeeded();

    // 4. API í˜¸ì¶œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
    const startTime = Date.now();
    const result = await this.executeWithRetry(optimizedPrompt, fullConfig);
    const responseTime = Date.now() - startTime;

    // 5. ê²°ê³¼ ì²˜ë¦¬ ë° ìºì‹±
    if (result.success && fullConfig.cacheStrategy !== 'none') {
      this.updateCache(cacheKey, result.data!, responseTime, fullConfig);
    }

    // 6. ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
    this.metrics.recordRequest(result, responseTime, fullConfig);

    return {
      ...result,
      metadata: {
        ...result.metadata,
        responseTime,
        fromCache: false,
      },
    };
  }

  /**
   * ë°°ì¹˜ ì²˜ë¦¬ (ì—¬ëŸ¬ ìš”ì²­ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬)
   */
  async generateBatch(
    requests: Array<{ prompt: string; config?: Partial<APIRequestConfig> }>
  ): Promise<APIResponse[]> {
    const batchSize = this.calculateOptimalBatchSize(requests.length);
    const results: APIResponse[] = [];

    for (let i = 0; i < requests.length; i += batchSize) {
      const batch = requests.slice(i, i + batchSize);
      const batchPromises = batch.map(req => this.generateContent(req.prompt, req.config));

      const batchResults = await Promise.allSettled(batchPromises);

      batchResults.forEach(result => {
        if (result.status === 'fulfilled') {
          results.push(result.value);
        } else {
          results.push({
            success: false,
            error: new Error(result.reason),
            metadata: {
              tokensUsed: 0,
              responseTime: 0,
              fromCache: false,
              creativity: 0,
              retryCount: 0,
            },
          });
        }
      });

      // ë°°ì¹˜ ê°„ ë”œë ˆì´ (ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ…)
      if (i + batchSize < requests.length) {
        await this.delay(this.rateLimiter.getBatchDelay());
      }
    }

    return results;
  }

  /**
   * ìŠ¤íŠ¸ë¦¬ë° ìƒì„± (ì‹¤ì‹œê°„ ì‘ë‹µ)
   */
  async *generateStream(
    prompt: string,
    config: Partial<APIRequestConfig> = {}
  ): AsyncGenerator<string, void, unknown> {
    const fullConfig = this.buildConfig(config);
    const optimizedPrompt = await this.optimizePrompt(prompt, fullConfig);

    await this.rateLimiter.waitIfNeeded();

    try {
      const result = await this.model.generateContentStream(optimizedPrompt);

      for await (const chunk of result.stream) {
        const chunkText = chunk.text();
        if (chunkText) {
          yield chunkText;
        }
      }
    } catch (_error) {
      throw new Error(`Streaming generation failed: ${error}`);
    }
  }

  /**
   * í”„ë¡¬í”„íŠ¸ ìµœì í™” (í† í° íš¨ìœ¨ì„± í–¥ìƒ)
   */
  private async optimizePrompt(prompt: string, config: APIRequestConfig): Promise<string> {
    // ì°½ì˜ì„± ëª¨ë“œì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì¡°ì •
    let optimizedPrompt = prompt;

    if (config.creativityMode === 'unlimited') {
      optimizedPrompt = this.enhanceCreativityPrompt(prompt);
    } else if (config.creativityMode === 'standard') {
      optimizedPrompt = this.compressPrompt(prompt);
    }

    // í† í° ì œí•œ í™•ì¸
    if (config.tokenLimit) {
      optimizedPrompt = await this.enforceTokenLimit(optimizedPrompt, config.tokenLimit);
    }

    return optimizedPrompt;
  }

  /**
   * ì°½ì˜ì„± í–¥ìƒ í”„ë¡¬í”„íŠ¸
   */
  private enhanceCreativityPrompt(prompt: string): string {
    const creativityBooster = `
ğŸ¨ UNLIMITED CREATIVITY MODE ACTIVATED
ë¹„ìš© ì‹ ê²½ì“°ì§€ ë§ê³  ë…ìë¥¼ ì™„ì „íˆ ë†€ë¼ê²Œ í•˜ì„¸ìš”!
ì—­ëŒ€ê¸‰ ë°˜ì „ê³¼ ê°ë™ì˜ ëª…ì¥ë©´ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

${prompt}

ì¶”ê°€ ì°½ì˜ì„± ì§€ì‹œì‚¬í•­:
- ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ ì„œìˆ  ê¸°ë²•ê³¼ í˜ì‹ ì  í‘œí˜„ ì‚¬ìš©
- ê°ì •ì˜ ê¹Šì´ë¥¼ ê·¹í•œê¹Œì§€ íŒŒê³ ë“œëŠ” ì‹¬ë¦¬ ë¬˜ì‚¬
- ì‹œì ì´ê³  ì˜í™”ì ì¸ ì¥ë©´ ì—°ì¶œ
- ë…ìê°€ "ì´ ì‘í’ˆì€ ì§„ì§œ ë‹¤ë¥´ë‹¤"ê³  ëŠë‚„ ë§Œí•œ ë…ì°½ì„±
`;
    return creativityBooster;
  }

  /**
   * í”„ë¡¬í”„íŠ¸ ì••ì¶• (í† í° ì ˆì•½)
   */
  private compressPrompt(prompt: string): string {
    return prompt
      .replace(/\n\s*\n/g, '\n') // ë¹ˆ ì¤„ ì œê±°
      .replace(/\s+/g, ' ') // ì—°ì† ê³µë°± ì••ì¶•
      .trim();
  }

  /**
   * í† í° ì œí•œ ê°•ì œ ì ìš©
   */
  private async enforceTokenLimit(prompt: string, limit: number): Promise<string> {
    // ê°„ë‹¨í•œ í† í° ì¶”ì • (ì‹¤ì œë¡œëŠ” ë” ì •í™•í•œ í† í° ê³„ì‚° í•„ìš”)
    const estimatedTokens = Math.ceil(prompt.length / 4);

    if (estimatedTokens <= limit) {
      return prompt;
    }

    // í”„ë¡¬í”„íŠ¸ ìë¥´ê¸° (ì¤‘ìš”í•œ ë¶€ë¶„ ë³´ì¡´)
    const ratio = limit / estimatedTokens;
    const targetLength = Math.floor(prompt.length * ratio * 0.9); // 10% ì—¬ìœ 

    return prompt.substring(0, targetLength) + '...';
  }

  /**
   * ì¬ì‹œë„ ë¡œì§ í¬í•¨ ì‹¤í–‰
   */
  private async executeWithRetry(prompt: string, config: APIRequestConfig): Promise<APIResponse> {
    let lastError: Error | null = null;

    for (let attempt = 0; attempt <= config.retryPolicy.maxRetries; attempt++) {
      try {
        const result = await this.model.generateContent(prompt);
        const response = await result.response;
        const text = response.text();

        // í›„ì²˜ë¦¬ (ì½”ë“œ ë¸”ë¡ ë§ˆì»¤ ì œê±° ë“±)
        const cleanedText = this.postProcessResponse(text);

        return {
          success: true,
          data: cleanedText,
          metadata: {
            tokensUsed: this.estimateTokensUsed(prompt, cleanedText),
            responseTime: 0, // ì™¸ë¶€ì—ì„œ ì„¤ì •ë¨
            fromCache: false,
            creativity: this.calculateCreativity(config.creativityMode),
            retryCount: attempt,
          },
        };
      } catch (_error) {
        lastError = _error as Error;

        if (
          !this.shouldRetry(_error as Error, config.retryPolicy) ||
          attempt === config.retryPolicy.maxRetries
        ) {
          break;
        }

        const delay = this.calculateRetryDelay(attempt, config.retryPolicy);
        await this.delay(delay);
      }
    }

    return {
      success: false,
      error: lastError || new Error('Unknown error'),
      metadata: {
        tokensUsed: 0,
        responseTime: 0,
        fromCache: false,
        creativity: 0,
        retryCount: config.retryPolicy.maxRetries,
      },
    };
  }

  /**
   * ì‘ë‹µ í›„ì²˜ë¦¬ (ê¸°ì¡´ gemini-story-generator.js ë¡œì§ ì ìš©)
   */
  private postProcessResponse(text: string): string {
    let content = text;

    // Gemini AI ì‘ë‹µì—ì„œ ì½”ë“œ ë¸”ë¡ ë§ˆì»¤ ì œê±°
    content = content.replace(/^```[\s\S]*?\n/, ''); // ì‹œì‘ ì½”ë“œ ë¸”ë¡ ì œê±°
    content = content.replace(/\n```\s*$/, ''); // ë ì½”ë“œ ë¸”ë¡ ì œê±°
    content = content.replace(/```\s*\n\s*```\s*$/, ''); // ë¹ˆ ì½”ë“œ ë¸”ë¡ ì œê±°
    content = content.trim(); // ì•ë’¤ ê³µë°± ì œê±°

    return content;
  }

  /**
   * ì„¤ì • ë¹Œë“œ
   */
  private buildConfig(config: Partial<APIRequestConfig>): APIRequestConfig {
    return {
      creativityMode: config.creativityMode || 'standard',
      priority: config.priority || 'normal',
      cacheStrategy: config.cacheStrategy || 'memory',
      retryPolicy: config.retryPolicy || this.getDefaultRetryPolicy(),
      tokenLimit: config.tokenLimit,
      timeoutMs: config.timeoutMs || 30000,
    };
  }

  private getDefaultRetryPolicy(): RetryPolicy {
    return {
      maxRetries: 3,
      baseDelayMs: 2000,
      exponentialBackoff: true,
      retryConditions: ['overloaded', '503', 'timeout'],
    };
  }

  /**
   * ìºì‹œ ê´€ë¦¬
   */
  private generateCacheKey(prompt: string, config: APIRequestConfig): string {
    const configStr = JSON.stringify({
      mode: config.creativityMode,
      limit: config.tokenLimit,
    });
    return `${this.hash(prompt)}-${this.hash(configStr)}`;
  }

  private checkCache(key: string, strategy: string): APIResponse | null {
    if (strategy === 'none') return null;

    const entry = this.cache.get(key);
    if (!entry) return null;

    const now = Date.now();
    if (now - entry.timestamp > entry.ttl) {
      this.cache.delete(key);
      return null;
    }

    entry.accessCount++;
    return {
      success: true,
      data: entry.content,
      metadata: {
        tokensUsed: entry.tokensUsed,
        responseTime: 0,
        fromCache: true,
        creativity: 0,
        retryCount: 0,
      },
    };
  }

  private updateCache(
    key: string,
    content: string,
    responseTime: number,
    config: APIRequestConfig
  ): void {
    const ttl =
      config.cacheStrategy === 'persistent'
        ? 24 * 60 * 60 * 1000 // 24ì‹œê°„
        : 60 * 60 * 1000; // 1ì‹œê°„

    this.cache.set(key, {
      content,
      timestamp: Date.now(),
      tokensUsed: this.estimateTokensUsed('', content),
      ttl,
      accessCount: 1,
    });

    // ìºì‹œ í¬ê¸° ì œí•œ
    if (this.cache.size > 1000) {
      this.evictLRUCache();
    }
  }

  /**
   * ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
   */
  private hash(str: string): string {
    let hash = 0;
    for (let i = 0; i < str.length; i++) {
      const char = str.charCodeAt(i);
      hash = (hash << 5) - hash + char;
      hash = hash & hash; // 32bit ì •ìˆ˜ë¡œ ë³€í™˜
    }
    return hash.toString(36);
  }

  private estimateTokensUsed(prompt: string, response: string): number {
    return Math.ceil((prompt.length + response.length) / 4);
  }

  private calculateCreativity(mode: string): number {
    const modeMap = {
      standard: 0.7,
      high: 0.85,
      unlimited: 1.0,
    };
    return modeMap[mode as keyof typeof modeMap] || 0.7;
  }

  private shouldRetry(error: Error, policy: RetryPolicy): boolean {
    return policy.retryConditions.some(condition =>
      error.message.toLowerCase().includes(condition.toLowerCase())
    );
  }

  private calculateRetryDelay(attempt: number, policy: RetryPolicy): number {
    if (!policy.exponentialBackoff) {
      return policy.baseDelayMs;
    }
    return policy.baseDelayMs * Math.pow(2, attempt);
  }

  private calculateOptimalBatchSize(requestCount: number): number {
    if (requestCount <= 5) return requestCount;
    if (requestCount <= 20) return 5;
    return 10;
  }

  private evictLRUCache(): void {
    // LRU ì •ì±…ìœ¼ë¡œ ìºì‹œ í•­ëª© ì œê±°
    const entries = Array.from(this.cache.entries());
    entries.sort((a, b) => a[1].accessCount - b[1].accessCount);

    const toDelete = entries.slice(0, 100); // 100ê°œ ì œê±°
    toDelete.forEach(([key]) => this.cache.delete(key));
  }

  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  /**
   * ë©”íŠ¸ë¦­ ì¡°íšŒ
   */
  getMetrics(): unknown {
    return this.metrics.getReport();
  }

  /**
   * ìºì‹œ í†µê³„
   */
  getCacheStats(): unknown {
    return {
      size: this.cache.size,
      hitRate: this.metrics.getCacheHitRate(),
      totalSavings: this.metrics.getTotalTokenSavings(),
    };
  }
}

// ë³´ì¡° í´ë˜ìŠ¤ë“¤
class RateLimiter {
  private lastRequestTime: number = 0;
  private requestCount: number = 0;
  private resetTime: number = 0;

  async waitIfNeeded(): Promise<void> {
    const now = Date.now();

    // ë¶„ë‹¹ ìš”ì²­ ì œí•œ (ì˜ˆ: 60 RPM)
    if (now - this.resetTime > 60000) {
      this.requestCount = 0;
      this.resetTime = now;
    }

    if (this.requestCount >= 60) {
      const waitTime = 60000 - (now - this.resetTime);
      await new Promise(resolve => setTimeout(resolve, waitTime));
    }

    // ìš”ì²­ ê°„ ìµœì†Œ ê°„ê²© (ì˜ˆ: 1ì´ˆ)
    const timeSinceLastRequest = now - this.lastRequestTime;
    if (timeSinceLastRequest < 1000) {
      await new Promise(resolve => setTimeout(resolve, 1000 - timeSinceLastRequest));
    }

    this.requestCount++;
    this.lastRequestTime = Date.now();
  }

  getBatchDelay(): number {
    return 2000; // ë°°ì¹˜ ê°„ 2ì´ˆ ë”œë ˆì´
  }
}

class APIMetrics {
  private requests: unknown[] = [];
  private cacheHits: number = 0;
  private cacheMisses: number = 0;

  recordRequest(result: APIResponse, responseTime: number, config: APIRequestConfig): void {
    this.requests.push({
      timestamp: Date.now(),
      success: result.success,
      responseTime,
      tokensUsed: result.metadata.tokensUsed,
      creativityMode: config.creativityMode,
      fromCache: result.metadata.fromCache,
    });

    if (result.metadata.fromCache) {
      this.cacheHits++;
    } else {
      this.cacheMisses++;
    }

    // ë©”íŠ¸ë¦­ í¬ê¸° ì œí•œ
    if (this.requests.length > 10000) {
      this.requests.splice(0, 5000);
    }
  }

  getCacheHitRate(): number {
    const total = this.cacheHits + this.cacheMisses;
    return total > 0 ? this.cacheHits / total : 0;
  }

  getTotalTokenSavings(): number {
    return this.requests
      .filter(req => req.fromCache)
      .reduce((total, req) => total + req.tokensUsed, 0);
  }

  getReport(): unknown {
    const recent = this.requests.slice(-100); // ìµœê·¼ 100ê°œ ìš”ì²­

    return {
      totalRequests: this.requests.length,
      successRate: recent.filter(r => r.success).length / Math.max(recent.length, 1),
      avgResponseTime:
        recent.reduce((sum, r) => sum + r.responseTime, 0) / Math.max(recent.length, 1),
      avgTokensUsed: recent.reduce((sum, r) => sum + r.tokensUsed, 0) / Math.max(recent.length, 1),
      cacheHitRate: this.getCacheHitRate(),
      totalTokenSavings: this.getTotalTokenSavings(),
    };
  }
}

interface QueuedRequest {
  prompt: string;
  config: APIRequestConfig;
  resolve: (value: APIResponse) => void;
  reject: (error: Error) => void;
}

export default EnhancedGeminiWrapper;
